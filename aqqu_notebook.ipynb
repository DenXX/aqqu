{
 "metadata": {
  "name": "",
  "signature": "sha256:155e3f15993799c89b6435ea0092710a69feb3313dfc461691809f970e8c51b9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle as pickle\n",
      "import gzip\n",
      "\n",
      "\n",
      "features_file = \"data/web/rank_dataset.pickle.gz\"\n",
      "labels = []\n",
      "features = []\n",
      "with gzip.open(features_file, 'r') as inp:\n",
      "    try:\n",
      "        index = 0\n",
      "        while True:\n",
      "            label, feature = pickle.load(inp)\n",
      "            labels.append(label)\n",
      "            features.append(feature)\n",
      "            index += 1\n",
      "            if index % 10000 == 0:\n",
      "                print(\"%d instances read\" % index)\n",
      "    except pickle.PickleError, EOFError:\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 instances read\n",
        "20000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100000 instances read"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-ca07a2acf20a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextrastart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextrabuf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextrasize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextrasize\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import time\n",
      "import operator\n",
      "import random\n",
      "import numpy as np\n",
      "from random import Random\n",
      "import cPickle as pickle\n",
      "from sklearn import utils\n",
      "from sklearn import metrics\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile\n",
      "from sklearn.externals import joblib\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \\\n",
      "    AdaBoostRegressor, RandomForestRegressor, ExtraTreesClassifier, GradientBoostingClassifier \n",
      "from sklearn import pipeline, grid_search\n",
      "from sklearn.linear_model import SGDClassifier, SGDRegressor, \\\n",
      "    LogisticRegressionCV, LogisticRegression\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder, \\\n",
      "    Normalizer, MinMaxScaler\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "def transform(features, labels):\n",
      "    label_encoder = LabelEncoder()\n",
      "    labels = label_encoder.fit_transform(labels)\n",
      "    vec = DictVectorizer(sparse=False)\n",
      "    X = vec.fit_transform(features)\n",
      "    X, labels = utils.shuffle(X, labels, random_state=999)\n",
      "    return X, labels\n",
      "\n",
      "def grid_cv(X, labels):\n",
      "    param_grid = {\"learning_rate\": [0.25, 0.5],\n",
      "                  \"max_depth\": [2, 3],\n",
      "                  \"max_features\": [1.0, ],\n",
      "                  \"min_samples_leaf\": [1, 5, 10],\n",
      "                  \"subsample\": [1.0],\n",
      "                  \"min_samples_split\":[2],\n",
      "                  }\n",
      "    decision_tree = GradientBoostingClassifier(n_estimators=1000)\n",
      "    cv = GridSearchCV(decision_tree, param_grid, verbose=2, n_jobs=10).fit(X, labels)\n",
      "    print cv.best_params_\n",
      "    return cv\n",
      "\n",
      "    #print \"Training trees...\"\n",
      "    #decision_tree.fit(X, labels)\n",
      "    #print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, labs = transform(features, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = grid_cv(X[:20000], labs[:20000])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=10)]: Done   1 jobs       | elapsed:  6.7min\n",
        "[Parallel(n_jobs=10)]: Done  36 out of  36 | elapsed: 22.6min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'learning_rate': 0.25, 'min_samples_leaf': 10, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 3}\n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 \n",
        "[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 - 6.7min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 - 8.1min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 - 6.8min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 - 6.7min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 - 6.8min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 - 6.7min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 - 6.4min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 - 6.4min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 - 6.9min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 - 9.1min\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 - 7.7min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 - 5.8min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 - 7.0min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 - 7.2min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 - 7.4min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 - 7.7min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 - 8.7min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 - 8.4min[CV]  max_features=1.0, learning_rate=0.25, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 - 6.6min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 - 5.7min\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 - 4.9min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 - 4.8min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 - 5.0min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=2 - 5.0min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 - 4.7min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=2 - 4.6min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 - 4.2min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 - 4.4min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=2 - 5.2min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=1, subsample=1.0, min_samples_split=2, max_depth=3 - 4.5min\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 [CV] max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 - 3.4min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 - 3.7min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=5, subsample=1.0, min_samples_split=2, max_depth=3 - 3.8min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 - 3.5min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 - 3.5min[CV]  max_features=1.0, learning_rate=0.5, min_samples_leaf=10, subsample=1.0, min_samples_split=2, max_depth=3 - 3.3min\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cv.best_score_\n",
      "cv.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.99285\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "[mean: 0.99165, std: 0.00019, params: {'learning_rate': 0.25, 'min_samples_leaf': 1, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 2},\n",
        " mean: 0.99215, std: 0.00046, params: {'learning_rate': 0.25, 'min_samples_leaf': 5, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 2},\n",
        " mean: 0.99265, std: 0.00053, params: {'learning_rate': 0.25, 'min_samples_leaf': 10, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 2},\n",
        " mean: 0.99220, std: 0.00032, params: {'learning_rate': 0.25, 'min_samples_leaf': 1, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 3},\n",
        " mean: 0.99215, std: 0.00078, params: {'learning_rate': 0.25, 'min_samples_leaf': 5, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 3},\n",
        " mean: 0.99285, std: 0.00060, params: {'learning_rate': 0.25, 'min_samples_leaf': 10, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 3},\n",
        " mean: 0.99185, std: 0.00046, params: {'learning_rate': 0.5, 'min_samples_leaf': 1, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 2},\n",
        " mean: 0.99270, std: 0.00050, params: {'learning_rate': 0.5, 'min_samples_leaf': 5, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 2},\n",
        " mean: 0.99240, std: 0.00111, params: {'learning_rate': 0.5, 'min_samples_leaf': 10, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 2},\n",
        " mean: 0.99140, std: 0.00102, params: {'learning_rate': 0.5, 'min_samples_leaf': 1, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 3},\n",
        " mean: 0.99265, std: 0.00012, params: {'learning_rate': 0.5, 'min_samples_leaf': 5, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 3},\n",
        " mean: 0.99235, std: 0.00044, params: {'learning_rate': 0.5, 'min_samples_leaf': 10, 'subsample': 1.0, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 3}]"
       ]
      }
     ],
     "prompt_number": 39
    }
   ],
   "metadata": {}
  }
 ]
}